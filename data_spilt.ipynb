{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6c4a17-e7d5-4a3a-ad1b-4ecb33199c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output_txt\\part_1.txt (3000 words)\n",
      "Saved: output_txt\\part_2.txt (3000 words)\n",
      "Saved: output_txt\\part_3.txt (3000 words)\n",
      "Saved: output_txt\\part_4.txt (3000 words)\n",
      "Saved: output_txt\\part_5.txt (3000 words)\n",
      "Saved: output_txt\\part_6.txt (3000 words)\n",
      "Saved: output_txt\\part_7.txt (3000 words)\n",
      "Saved: output_txt\\part_8.txt (3000 words)\n",
      "Saved: output_txt\\part_9.txt (3000 words)\n",
      "Saved: output_txt\\part_10.txt (3000 words)\n",
      "Saved: output_txt\\part_11.txt (3000 words)\n",
      "Saved: output_txt\\part_12.txt (3000 words)\n",
      "Saved: output_txt\\part_13.txt (3000 words)\n",
      "Saved: output_txt\\part_14.txt (3000 words)\n",
      "Saved: output_txt\\part_15.txt (3000 words)\n",
      "Saved: output_txt\\part_16.txt (3000 words)\n",
      "Saved: output_txt\\part_17.txt (3000 words)\n",
      "Saved: output_txt\\part_18.txt (1434 words)\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import os\n",
    "\n",
    "def split_docx_to_txt(input_file, output_dir, words_per_file=3000):\n",
    "    # Load the DOCX file\n",
    "    doc = Document(input_file)\n",
    "    \n",
    "    # Extract all text\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            full_text.append(para.text.strip())\n",
    "    \n",
    "    # Join paragraphs into one big text\n",
    "    all_text = \" \".join(full_text)\n",
    "    \n",
    "    # Split text into words\n",
    "    words = all_text.split()\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Split into chunks of words_per_file\n",
    "    file_count = 1\n",
    "    for i in range(0, len(words), words_per_file):\n",
    "        chunk_words = words[i:i + words_per_file]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        \n",
    "        output_path = os.path.join(output_dir, f\"part_{file_count}.txt\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(chunk_text)\n",
    "        \n",
    "        print(f\"Saved: {output_path} ({len(chunk_words)} words)\")\n",
    "        file_count += 1\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_docx = \"Books/G10-Ps-Tapfseer_extracted.docx\"   # Replace with your Pashto docx filename\n",
    "    output_folder = \"output_txt\"      # Folder where txt files will be saved\n",
    "    \n",
    "    split_docx_to_txt(input_docx, output_folder, words_per_file=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4535a4-fc50-4c69-8a69-4cc4333a672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: txtfile/output_txt/Tapfseer\\part_1.txt (2826 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_2.txt (2713 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_3.txt (2719 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_4.txt (2934 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_5.txt (2782 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_6.txt (2684 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_7.txt (2825 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_8.txt (2676 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_9.txt (2936 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_10.txt (2790 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_11.txt (2881 words)\n",
      "Saved: txtfile/output_txt/Tapfseer\\part_12.txt (1676 words)\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import os\n",
    "\n",
    "def split_docx_to_txt(input_file, output_dir, words_per_file=3000):\n",
    "    # Load the DOCX file\n",
    "    doc = Document(input_file)\n",
    "\n",
    "    # Extract paragraphs (keep empty lines between them)\n",
    "    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    file_count = 1\n",
    "    word_count = 0\n",
    "    chunk_paragraphs = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para_words = para.split()\n",
    "        \n",
    "        # If adding this paragraph exceeds the limit, save the current chunk\n",
    "        if word_count + len(para_words) > words_per_file and chunk_paragraphs:\n",
    "            output_path = os.path.join(output_dir, f\"part_{file_count}.txt\")\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\\n\".join(chunk_paragraphs))  # preserve paragraph breaks\n",
    "            \n",
    "            print(f\"Saved: {output_path} ({word_count} words)\")\n",
    "            file_count += 1\n",
    "            chunk_paragraphs = []\n",
    "            word_count = 0\n",
    "\n",
    "        # Add paragraph to the current chunk\n",
    "        chunk_paragraphs.append(para)\n",
    "        word_count += len(para_words)\n",
    "\n",
    "    # Save any remaining paragraphs\n",
    "    if chunk_paragraphs:\n",
    "        output_path = os.path.join(output_dir, f\"part_{file_count}.txt\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\\n\".join(chunk_paragraphs))\n",
    "        print(f\"Saved: {output_path} ({word_count} words)\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_docx = \"Books/G10-Ps-Tapfseer_extracted.docx\"   # Replace with your Pashto docx filename\n",
    "    output_folder = \"txtfile/output_txt/Tapfseer\"      # Folder where txt files will be saved\n",
    "    \n",
    "    split_docx_to_txt(input_docx, output_folder, words_per_file=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f38fa-f1a6-4757-a626-3b854e8a1271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
